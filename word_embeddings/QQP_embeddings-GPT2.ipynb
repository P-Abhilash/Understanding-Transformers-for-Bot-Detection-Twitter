{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "64BRoc77oJJ4",
    "outputId": "e2ba3121-d8b1-4780-91c9-afbaef9a4b81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, AdamW, BertTokenizer, BertForSequenceClassification, BertModel, get_linear_schedule_with_warmup\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLrvuccKoMge"
   },
   "outputs": [],
   "source": [
    "directory = \"../data/glue_data/QQP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1Cz6fQtordd"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(directory+\"train.tsv\", sep = \"\\t\",  quoting = 3)\n",
    "test = pd.read_csv(directory+\"test.tsv\", sep = \"\\t\",   quoting = 3)\n",
    "dev = pd.read_csv(directory+\"dev.tsv\", sep = \"\\t\",   quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lCNDd2rrLVU"
   },
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "dev = dev.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PL-8V2SbpAYr"
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHsIqoyTpCv0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cl2N33LpEEi"
   },
   "outputs": [],
   "source": [
    "if model_name == \"BERT\":\n",
    "    train_sentences = train[3].values\n",
    "    #train_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in train_sentences]\n",
    "    train_labels = train[1].values\n",
    "\n",
    "    test_sentences = dev[3].values\n",
    "    #test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
    "    test_labels = dev[1].values\n",
    "    \n",
    "elif model_name == \"gpt2\":\n",
    "    train_sentences_a = train['question1'].values\n",
    "    train_sentences_b = train['question2'].values\n",
    "    train_labels = train['is_duplicate'].values\n",
    "    \n",
    "    test_sentences_a = dev['question1'].values\n",
    "    test_sentences_b = dev['question2'].values\n",
    "    test_labels = dev['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soRgVp41pIM_"
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "if model_name == \"BERT\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "elif model_name == \"gpt2\":\n",
    "    special_tokens_dict = {'bos_token':'_start_','sep_token':'[SEP]', 'cls_token': '[CLS]'}\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', do_lower_case=True)\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsSOcx9rpLnk"
   },
   "outputs": [],
   "source": [
    "start = tokenizer.convert_tokens_to_ids(\"_start_\")\n",
    "classify = tokenizer.convert_tokens_to_ids(\"[CLS]\")\n",
    "sep = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "\n",
    "train_sentences_a_tokens = [tokenizer.encode(sent_a) for sent_a in train_sentences_a]\n",
    "train_sentences_b_tokens = [tokenizer.encode(sent_b) for sent_b in train_sentences_b]\n",
    "\n",
    "test_sentences_a_tokens = [tokenizer.encode(sent_a) for sent_a in test_sentences_a]\n",
    "test_sentences_b_tokens = [tokenizer.encode(sent_b) for sent_b in test_sentences_b]\n",
    "\n",
    "train_input_ids = [[start] + sent_a[:max_length-3] + [sep] + sent_b[:(max_length-(3+len(sent_a[:max_length-3])))] + [classify] for sent_a,sent_b in zip(train_sentences_a_tokens,train_sentences_b_tokens)]\n",
    "test_input_ids = [[start] + sent_a[:max_length-3] + [sep] + sent_b[:(max_length-(3+len(sent_a[:max_length-3])))] + [classify] for sent_a,sent_b in zip(test_sentences_a_tokens,test_sentences_b_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5t7WXBVsPfb"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7PEOYWrsRqX"
   },
   "outputs": [],
   "source": [
    "cls_position_train = np.argmax(train_input_ids==classify,axis=1)\n",
    "cls_position_test = np.argmax(test_input_ids==classify,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jq5cQcpUsTwp"
   },
   "outputs": [],
   "source": [
    "train_attention_masks = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for seq in train_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    train_attention_masks.append(seq_mask)\n",
    "    \n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "r4vHnp1QsVkP",
    "outputId": "f1793823-0c54-4b99-e793-f40741a02419"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_inputs = torch.tensor(train_input_ids)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "train_labels = torch.tensor(train_labels).long()\n",
    "train_cls_positions= torch.tensor(cls_position_train)\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_labels = torch.tensor(test_labels).long()\n",
    "test_cls_positions=torch.tensor(cls_position_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLdmqE7MsYPn"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_cls_positions)\n",
    "#train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_cls_positions)\n",
    "#test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CneOhtqWsbI8"
   },
   "outputs": [],
   "source": [
    "if model_name == \"BERT\":\n",
    "    #model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model = torch.load('../models/COLA-finetuned-BERT-v2.pt')\n",
    "elif model_name == \"gpt2\":\n",
    "    #model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\", num_labels = 2)\n",
    "    model = torch.load('../models/GPT2_QQP/model.bin')\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1qARy7WszDM"
   },
   "outputs": [],
   "source": [
    "linear = nn.Linear(128*768, 2).to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8Op5Ajus05t"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(linear.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*3)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkCYVrL7s3PZ"
   },
   "outputs": [],
   "source": [
    "if model_name==\"BERT\":\n",
    "  model.bert.config.output_hidden_states = True\n",
    "  model.bert.config.is_decoder = False\n",
    "  model.bert.encoder.output_hidden_states = True\n",
    "  for i in range(0,len(model.bert.encoder.layer)): \n",
    "    model.bert.encoder.layer[i].is_decoder = False\n",
    "    model.bert.encoder.layer[i].output_hidden_states = True\n",
    "else:\n",
    "    model.transformer.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "StYoaZTLs53M",
    "outputId": "76f6fa13-7e3e-48b8-b07b-c17917cc3192"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [2:58:38<00:00, 3572.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "    \n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_masks, b_labels, b_cls_positions = batch\n",
    "    #b_input_ids, b_masks, b_labels = batch\n",
    "    optimizer.zero_grad()\n",
    "    if(model_name==\"gpt2\"):\n",
    "        outputs = model.transformer(b_input_ids, attention_mask=b_masks)\n",
    "        h0 = outputs[2][0] ## Here we are using embeddings layer \n",
    "        logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "    else:\n",
    "        outputs = model.bert(b_input_ids, attention_mask=b_masks)\n",
    "        h0 = outputs[2][0] ## Here we are using embeddings layer\n",
    "        logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss=loss_fct(logits.view(-1, logits.size(-1)),\n",
    "                          b_labels.view(-1))\n",
    "    train_loss_set.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yF9cfKrw2Ii"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "linear.eval()\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_masks, b_labels, b_cls_positions = batch\n",
    "    #b_input_ids, b_masks, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        if(model_name==\"gpt2\"):\n",
    "            outputs = model.transformer(b_input_ids, attention_mask=b_masks)\n",
    "            h0 = outputs[2][0] ## Here we are using embeddings layer \n",
    "            logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "        else:\n",
    "            outputs = outputs = model.bert(b_input_ids, attention_mask=b_masks)\n",
    "            h0 = outputs[2][0] ## Here we are using embeddings layer \n",
    "            logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "   \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        preds.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "GsFKF2K4w84S",
    "outputId": "3a425c21-6db9-4226-c47a-508ac2537070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8137    0.7552    0.7834     25545\n",
      "           1     0.6261    0.7033    0.6624     14885\n",
      "\n",
      "    accuracy                         0.7361     40430\n",
      "   macro avg     0.7199    0.7293    0.7229     40430\n",
      "weighted avg     0.7446    0.7361    0.7389     40430\n",
      "\n",
      "Accuracy:  0.736111798169676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = []\n",
    "for pred in preds:\n",
    "    p = np.argmax(pred, axis = 1)\n",
    "    for label in p:\n",
    "        predictions.append(label)\n",
    "print(classification_report(test_labels, predictions, digits = 4))\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2-QQP-Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
