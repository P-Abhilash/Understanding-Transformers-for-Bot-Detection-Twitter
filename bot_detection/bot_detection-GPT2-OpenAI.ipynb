{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from analysis import SST_analyze\n",
    "from analysis import news as news_analysis\n",
    "from datasets import SST2\n",
    "from loss import ClassificationLossCompute\n",
    "from model_pytorch import LMHead, load_openai_pretrained_model, MLP\n",
    "from opt import OpenAIAdam\n",
    "from text_utils import TextEncoder\n",
    "from utils import (encode_dataset, iter_data,\n",
    "                   ResultLogger, make_path)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "def transform_news(X1):\n",
    "    n_batch = len(X1)\n",
    "    xmb = np.zeros((n_batch, n_ctx, 2), dtype=np.int32)\n",
    "    mmb = np.zeros((n_batch, n_ctx), dtype=np.float32)\n",
    "\n",
    "    #start = encoder['_start_']\n",
    "    for i, x1 in enumerate(X1):\n",
    "        print(max_len, clf_token)\n",
    "        x12 = x1[:max_len] + [clf_token]\n",
    "        l12 = len(x12)\n",
    "        xmb[i, :l12, 0] = x12\n",
    "        mmb[i, :l12] = 1\n",
    "    # Position information that is added to the input embeddings in the TransformerModel\n",
    "    xmb[:, :, 1] = np.arange(n_vocab + n_special, n_vocab + n_special + n_ctx)\n",
    "    return xmb, mmb\n",
    "\n",
    "\n",
    "def iter_apply(Xs, Ms, Ys):\n",
    "    # fns = [lambda x: np.concatenate(x, 0), lambda x: float(np.sum(x))]\n",
    "    logits = []\n",
    "    cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dh_model.eval()\n",
    "        for xmb, mmb, ymb in iter_data(Xs, Ms, Ys, n_batch=n_batch_train, truncate=False, verbose=True):\n",
    "            n = len(xmb)\n",
    "            XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n",
    "            YMB = torch.tensor(ymb, dtype=torch.long).to(device)\n",
    "            MMB = torch.tensor(mmb).to(device)\n",
    "            _, clf_logits = dh_model(XMB[..., 0])\n",
    "            # clf_logits *= n\n",
    "            clf_losses = compute_loss_fct(XMB, YMB, MMB, clf_logits, only_return_losses=True)\n",
    "            # clf_losses *= n\n",
    "            logits.append(clf_logits.to(\"cpu\").numpy())\n",
    "            cost += clf_losses.sum().item()\n",
    "        logits = np.concatenate(logits, 0)\n",
    "    return logits, cost\n",
    "\n",
    "\n",
    "def iter_predict(Xs, Ms):\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        dh_model.eval()\n",
    "        for xmb, mmb in iter_data(Xs, Ms, n_batch=n_batch_train, truncate=False, verbose=True):\n",
    "            n = len(xmb)\n",
    "            XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n",
    "            MMB = torch.tensor(mmb).to(device)\n",
    "            _, clf_logits = dh_model(XMB[..., 0])\n",
    "            logits.append(clf_logits.to(\"cpu\").numpy())\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def log(save_dir, desc):\n",
    "    global best_score\n",
    "    print(\"Logging\")\n",
    "    tr_logits, tr_cost = iter_apply(trX[:n_valid], trM[:n_valid], trY[:n_valid])\n",
    "    va_logits, va_cost = iter_apply(vaX, vaM, vaY)\n",
    "    tr_cost = tr_cost / len(trY[:n_valid])\n",
    "    va_cost = va_cost / n_valid\n",
    "    tr_acc = accuracy_score(trY[:n_valid], np.argmax(tr_logits, 1)) * 100.\n",
    "    va_acc = accuracy_score(vaY, np.argmax(va_logits, 1)) * 100.\n",
    "    logger.log(n_epochs=n_epochs, n_updates=n_updates, tr_cost=tr_cost, va_cost=va_cost, tr_acc=tr_acc, va_acc=va_acc)\n",
    "    print('%d %d %.3f %.3f %.2f %.2f' % (n_epochs, n_updates, tr_cost, va_cost, tr_acc, va_acc))\n",
    "    if submit:\n",
    "        score = va_acc\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            path = os.path.join(save_dir, desc, 'best_params')\n",
    "            torch.save(dh_model.state_dict(), make_path(path))\n",
    "\n",
    "\n",
    "def predict(dataset, submission_dir):\n",
    "    filename = filenames[dataset]\n",
    "    pred_fn = pred_fns[dataset]\n",
    "    label_decoder = label_decoders[dataset]\n",
    "    predictions = pred_fn(iter_predict(teX, teM))\n",
    "    if label_decoder is not None:\n",
    "        predictions = [label_decoder[prediction] for prediction in predictions]\n",
    "    path = os.path.join(submission_dir, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('{}\\t{}\\n'.format('index', 'prediction'))\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            f.write('{}\\t{}\\n'.format(i, prediction))\n",
    "\n",
    "\n",
    "def run_epoch():\n",
    "    for xmb, mmb, ymb in iter_data(*(trX, trM, trYt),\n",
    "                                   n_batch=n_batch_train, truncate=True, verbose=True):\n",
    "        #shuffle,random_state=np.random\n",
    "        global n_updates\n",
    "        dh_model.train()\n",
    "        XMB = torch.tensor(xmb, dtype=torch.long).to(device)  # torch.Size([4,257,2])\n",
    "        YMB = torch.tensor(ymb, dtype=torch.long).to(device)  # torch.Size([4])\n",
    "        MMB = torch.tensor(mmb).to(device)  # ndarray: size(1028)\n",
    "        lm_logits, clf_logits = dh_model(XMB[..., 0])\n",
    "        # lm_logits: torch.Size([1024, 40737])\n",
    "        # clf_logits: torch.Size([4, 2])\n",
    "        compute_loss_fct(XMB, YMB, MMB, clf_logits, lm_logits)\n",
    "        n_updates += 1\n",
    "        if n_updates in [1000, 2000, 4000, 8000, 16000, 32000] and n_epochs == 0:\n",
    "            log(save_dir, desc)\n",
    "\n",
    "\n",
    "argmax = lambda x: np.argmax(x, 1)\n",
    "\n",
    "pred_fns = {\n",
    "    'sst': argmax\n",
    "}\n",
    "\n",
    "filenames = {\n",
    "    'sst': 'sst'\n",
    "}\n",
    "\n",
    "label_decoders = {\n",
    "    'sst': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClfHead(nn.Module):\n",
    "    \"\"\"Classification Head for the transformer\n",
    "\n",
    "    TODO: test this class.\"\"\"\n",
    "\n",
    "    def __init__(self, clf_token, cfg, n_class):\n",
    "        super(ClfHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = nn.Dropout(cfg.clf_pdrop)\n",
    "        self.linear = nn.Linear(cfg.n_embd, n_class)\n",
    "        self.mlp = MLP(4 * cfg.n_embd, cfg)\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        # h: Tensor [4,257,768]\n",
    "        #print(h.shape)\n",
    "        clf_h = h.view(-1, self.n_embd)\n",
    "        #print(clf_h.shape)\n",
    "        # clf_h: tensor[1028,768]\n",
    "        #flat = x[..., 0].contiguous().view(-1)\n",
    "        flat= x.contiguous().view(-1)\n",
    "        #print(flat.shape)\n",
    "        # falt: tensor[1028]\n",
    "        #print(self.clf_token)\n",
    "        clf_h = clf_h[flat == self.clf_token, :]\n",
    "        # clf_h: tensor: [4,768]\n",
    "        clf_h = self.dropout(clf_h)\n",
    "        clf_logits = self.linear(clf_h)  # Tensor [4,2]\n",
    "\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
    "n_vocab = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class DoubleHeadModel(nn.Module):\n",
    "    \"\"\" Transformer with language model and task specific heads \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, clf_token, task_head_type, vocab=40990, n_ctx=512):\n",
    "        super(DoubleHeadModel, self).__init__()\n",
    "        #self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n",
    "        self.transformer = GPT2Model.from_pretrained('gpt2')\n",
    "        self.transformer.resize_token_embeddings(len(tokenizer))\n",
    "        self.lm_head = LMHead(self.transformer, cfg)\n",
    "        if isinstance(task_head_type, str):\n",
    "            if task_head_type == 'multiple_choice':\n",
    "                self.task_head = MultipleChoiceHead(clf_token, cfg)\n",
    "            elif task_head_type == 'similarity':\n",
    "                self.task_head = SimilarityHead(clf_token, cfg)\n",
    "            elif task_head_type == 'inference':\n",
    "                # the three classes correspond to entailment, contradiction and neutral.\n",
    "                self.task_head = ClfHead(clf_token, cfg, 3)\n",
    "            else:\n",
    "                raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
    "                                 \"'similarity', 'inference' or ('classification', n_class) \"\n",
    "                                 \"got {task_head_type}.\")\n",
    "        elif isinstance(task_head_type, collections.abc.Sequence) and len(task_head_type) == 2 and \\\n",
    "                task_head_type[0] == 'classification':\n",
    "            n_class = task_head_type[1]\n",
    "            self.task_head = ClfHead(clf_token, cfg, n_class)\n",
    "        else:\n",
    "            raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
    "                             \"'similarity', 'inference' or ('classification', n_class) \"\n",
    "                             \"got {task_head_type}.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.transformer(x)  # [4,257,768]\n",
    "        h = h[0]\n",
    "        #print(h.shape)\n",
    "        # h: torch.float32, torch.Size([4, 257, 768])\n",
    "        lm_logits = self.lm_head(h)  # [1024,40737]\n",
    "        # lm_logits: torch.float32, torch.Size([1024, 40737])\n",
    "        sum = torch.sum((x!=0), dim=1)\n",
    "        task_logits = self.task_head(h, x)  # [4,2]\n",
    "        # task_logits: torch.Size([4, 2])\n",
    "        return lm_logits, task_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda n_gpu: 1\n",
      "Encoding dataset...\n"
     ]
    }
   ],
   "source": [
    "submit = True\n",
    "dataset = 'sst'\n",
    "#n_ctx = 52\n",
    "n_ctx = 51\n",
    "save_dir = '../models/save_gpt2/'\n",
    "desc = 'sst'\n",
    "data_dir = '../data/bot_detection'\n",
    "log_dir = 'log/'\n",
    "submission_dir = 'submission_gpt2/'\n",
    "\n",
    "np.random.seed(2345)\n",
    "# set random seed for all CPU\n",
    "torch.manual_seed(2345)\n",
    "# set random seed for all GPU\n",
    "torch.cuda.manual_seed_all(2345)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(device)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"device:\", device, \"n_gpu:\", n_gpu)\n",
    "# device: cuda n_gpu: 1\n",
    "\n",
    "#logger = ResultLogger(path=os.path.join(log_dir, '{}.jsonl'.format(desc)), **args.__dict__)\n",
    "#text_encoder = TextEncoder('model/encoder_bpe_40000.json', 'model/vocab_40000.bpe')\n",
    "#encoder = text_encoder.encoder\n",
    "#n_vocab = len(text_encoder.encoder)\n",
    "\n",
    "print(\"Encoding dataset...\")\n",
    "\n",
    "#((trX1, trY),\n",
    "# (vaX1, vaY),\n",
    "# (teX1,)) = encode_dataset(*SST2(data_dir),\n",
    "#                           encoder=text_encoder)\n",
    "\n",
    "trX1 = [tokenizer.encode(sentence) for sentence in SST2(data_dir)[0][0]]\n",
    "trY = [label for label in SST2(data_dir)[0][1]]\n",
    "vaX1 = [tokenizer.encode(sentence) for sentence in SST2(data_dir)[1][0]]\n",
    "vaY = [label for label in SST2(data_dir)[1][1]]\n",
    "teX1 =[tokenizer.encode(sentence) for sentence in SST2(data_dir)[2][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder['_start_'] = len(encoder)\n",
    "#encoder['_classify_'] = len(encoder)\n",
    "#clf_token = encoder['_classify_']\n",
    "clf_token = tokenizer.cls_token_id\n",
    "#n_special = 2\n",
    "n_special = 1\n",
    "#max_len = n_ctx - 2\n",
    "max_len = n_ctx - 1\n",
    "# n_ctx is the maximum number of token in an input sequence\n",
    "n_ctx = min(max(\n",
    "#    [len(x1[:max_len]) for x1 in trX1]\n",
    "#    + [len(x1[:max_len]) for x1 in vaX1]\n",
    "     [len(x1[:max_len]) for x1 in teX1]\n",
    ") + 2, n_ctx)\n",
    "\n",
    "vocab = n_vocab + n_special + n_ctx\n",
    "trX, trM = transform_news(trX1)\n",
    "vaX, vaM = transform_news(vaX1)\n",
    "if submit:\n",
    "    teX, teM = transform_news(teX1)\n",
    "\n",
    "n_train = len(trY)\n",
    "n_valid = len(vaY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch_train = 8 * max(n_gpu, 1)\n",
    "n_updates_total = (500000 // n_batch_train) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\"n_embd\":768,\"embd_pdrop\":0.1,\"n_layer\":12,\"n_ctx\":51,\"clf_pdrop\":0.1,\"n_head\":12,\"attn_pdrop\":0.1,\"resid_pdrop\":0.1,\"afn\":\"gelu\",\"submission_dir\":\"submission_gpt2\"}\n",
    "args=AttrDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ResultLogger(path=os.path.join(log_dir, '{}.jsonl'.format(desc)), **args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/bots/lib/python3.8/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "dh_model = DoubleHeadModel(args, clf_token, ('classification', 2), vocab, n_ctx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "model_opt = OpenAIAdam(dh_model.parameters(),\n",
    "                       lr=6.25e-5,\n",
    "                       schedule='warmup_linear',\n",
    "                       warmup=0.002,\n",
    "                       t_total=n_updates_total,\n",
    "                       b1=0.9,\n",
    "                       b2=0.999,\n",
    "                       e=1e-8,  # epsilon\n",
    "                       l2=0.01,\n",
    "                       vector_l2='store_true',\n",
    "                       max_grad_norm=1)\n",
    "compute_loss_fct = ClassificationLossCompute(criterion,\n",
    "                                             criterion,\n",
    "                                             0.5,\n",
    "                                             model_opt)\n",
    "#load_openai_pretrained_model(dh_model.transformer, n_ctx=n_ctx, n_special=n_special)\n",
    "\n",
    "dh_model.to(device)\n",
    "dh_model = nn.DataParallel(dh_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_updates = 0\n",
    "n_epochs = 0\n",
    "if dataset != 'stsb':\n",
    "    trYt = trY\n",
    "if submit:\n",
    "    path = os.path.join(save_dir, desc, 'best_params')\n",
    "    torch.save(dh_model.state_dict(), make_path(path))\n",
    "best_score = 0\n",
    "for i in range(3):\n",
    "    print(\"running epoch\", i)\n",
    "    run_epoch()\n",
    "    n_epochs += 1\n",
    "    log(save_dir, desc)\n",
    "if submit:\n",
    "    path = os.path.join(save_dir, desc, 'best_params')\n",
    "    dh_model.load_state_dict(torch.load(path))\n",
    "    predict(dataset, args.submission_dir)\n",
    "    if args.analysis:\n",
    "        SST_analyze(data_dir, os.path.join(args.submission_dir, filenames[dataset]),\n",
    "                    os.path.join(log_dir, '{}.jsonl'.format(dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bots",
   "language": "python",
   "name": "bots"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
