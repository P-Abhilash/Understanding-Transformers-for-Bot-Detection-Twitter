{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to see if the intuition found in the paper, that when finetuning BERT the cosine similarity is close to one, while GPT2 is not that much, allowing GPT2 to perform better than BERT in the bot classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = '../data/bot_detection/'\n",
    "test = pd.read_csv(directory + \"test.csv\", header=None)\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'id':range(len(test)),\n",
    "    'label':test[0],\n",
    "    'mark':['a']*test.shape[0],\n",
    "    'text': test[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "test.columns =  [\"index\", \"label\", \"mark\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BERT\"\n",
    "\n",
    "if model_name == \"BERT\":\n",
    "    test_sentences = test.tweet.values\n",
    "    test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
    "    test_labels = test.label.values\n",
    "\n",
    "elif model_name == \"gpt2\":\n",
    "    test_sentences = test.tweet.values\n",
    "    test_labels = test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "if model_name == \"BERT\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "elif model_name == \"gpt2\":\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', do_lower_case=True)\n",
    "\n",
    "test_input_ids = [tokenizer.encode(sent) for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 128\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(test_input_ids).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "batch_size=200\n",
    "test_data = TensorDataset(test_inputs)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"BERT\":\n",
    "    #model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model = torch.load(\"../models/BERT_Classifier_Large.pt\")\n",
    "    model = model.cuda()\n",
    "    model = model.bert\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.is_decoder = False\n",
    "    model.encoder.output_hidden_states = True\n",
    "    for i in range(0,len(model.encoder.layer)): \n",
    "        model.encoder.layer[i].is_decoder = False\n",
    "        model.encoder.layer[i].output_hidden_states = True\n",
    "elif model_name == \"gpt2\":\n",
    "    #model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "    model = torch.load(\"../models/Gpt2_Classifier_Large.pt\")\n",
    "    model = model.cuda()\n",
    "    model = model.transformer\n",
    "    model.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 500\n",
      "1 / 500\n",
      "2 / 500\n",
      "3 / 500\n",
      "4 / 500\n",
      "5 / 500\n",
      "6 / 500\n",
      "7 / 500\n",
      "8 / 500\n",
      "9 / 500\n",
      "10 / 500\n",
      "11 / 500\n",
      "12 / 500\n",
      "13 / 500\n",
      "14 / 500\n",
      "15 / 500\n",
      "16 / 500\n",
      "17 / 500\n",
      "18 / 500\n",
      "19 / 500\n",
      "20 / 500\n",
      "21 / 500\n",
      "22 / 500\n",
      "23 / 500\n",
      "24 / 500\n",
      "25 / 500\n",
      "26 / 500\n",
      "27 / 500\n",
      "28 / 500\n",
      "29 / 500\n",
      "30 / 500\n",
      "31 / 500\n",
      "32 / 500\n",
      "33 / 500\n",
      "34 / 500\n",
      "35 / 500\n",
      "36 / 500\n",
      "37 / 500\n",
      "38 / 500\n",
      "39 / 500\n",
      "40 / 500\n",
      "41 / 500\n",
      "42 / 500\n",
      "43 / 500\n",
      "44 / 500\n",
      "45 / 500\n",
      "46 / 500\n",
      "47 / 500\n",
      "48 / 500\n",
      "49 / 500\n",
      "50 / 500\n",
      "51 / 500\n",
      "52 / 500\n",
      "53 / 500\n",
      "54 / 500\n",
      "55 / 500\n",
      "56 / 500\n",
      "57 / 500\n",
      "58 / 500\n",
      "59 / 500\n",
      "60 / 500\n",
      "61 / 500\n",
      "62 / 500\n",
      "63 / 500\n",
      "64 / 500\n",
      "65 / 500\n",
      "66 / 500\n",
      "67 / 500\n",
      "68 / 500\n",
      "69 / 500\n",
      "70 / 500\n",
      "71 / 500\n",
      "72 / 500\n",
      "73 / 500\n",
      "74 / 500\n",
      "75 / 500\n",
      "76 / 500\n",
      "77 / 500\n",
      "78 / 500\n",
      "79 / 500\n",
      "80 / 500\n",
      "81 / 500\n",
      "82 / 500\n",
      "83 / 500\n",
      "84 / 500\n",
      "85 / 500\n",
      "86 / 500\n",
      "87 / 500\n",
      "88 / 500\n",
      "89 / 500\n",
      "90 / 500\n",
      "91 / 500\n",
      "92 / 500\n",
      "93 / 500\n",
      "94 / 500\n",
      "95 / 500\n",
      "96 / 500\n",
      "97 / 500\n",
      "98 / 500\n",
      "99 / 500\n",
      "100 / 500\n",
      "101 / 500\n",
      "102 / 500\n",
      "103 / 500\n",
      "104 / 500\n",
      "105 / 500\n",
      "106 / 500\n",
      "107 / 500\n",
      "108 / 500\n",
      "109 / 500\n",
      "110 / 500\n",
      "111 / 500\n",
      "112 / 500\n",
      "113 / 500\n",
      "114 / 500\n",
      "115 / 500\n",
      "116 / 500\n",
      "117 / 500\n",
      "118 / 500\n",
      "119 / 500\n",
      "120 / 500\n",
      "121 / 500\n",
      "122 / 500\n",
      "123 / 500\n",
      "124 / 500\n",
      "125 / 500\n",
      "126 / 500\n",
      "127 / 500\n",
      "128 / 500\n",
      "129 / 500\n",
      "130 / 500\n",
      "131 / 500\n",
      "132 / 500\n",
      "133 / 500\n",
      "134 / 500\n",
      "135 / 500\n",
      "136 / 500\n",
      "137 / 500\n",
      "138 / 500\n",
      "139 / 500\n",
      "140 / 500\n",
      "141 / 500\n",
      "142 / 500\n",
      "143 / 500\n",
      "144 / 500\n",
      "145 / 500\n",
      "146 / 500\n",
      "147 / 500\n",
      "148 / 500\n",
      "149 / 500\n",
      "150 / 500\n",
      "151 / 500\n",
      "152 / 500\n",
      "153 / 500\n",
      "154 / 500\n",
      "155 / 500\n",
      "156 / 500\n",
      "157 / 500\n",
      "158 / 500\n",
      "159 / 500\n",
      "160 / 500\n",
      "161 / 500\n",
      "162 / 500\n",
      "163 / 500\n",
      "164 / 500\n",
      "165 / 500\n",
      "166 / 500\n",
      "167 / 500\n",
      "168 / 500\n",
      "169 / 500\n",
      "170 / 500\n",
      "171 / 500\n",
      "172 / 500\n",
      "173 / 500\n",
      "174 / 500\n",
      "175 / 500\n",
      "176 / 500\n",
      "177 / 500\n",
      "178 / 500\n",
      "179 / 500\n",
      "180 / 500\n",
      "181 / 500\n",
      "182 / 500\n",
      "183 / 500\n",
      "184 / 500\n",
      "185 / 500\n",
      "186 / 500\n",
      "187 / 500\n",
      "188 / 500\n",
      "189 / 500\n",
      "190 / 500\n",
      "191 / 500\n",
      "192 / 500\n",
      "193 / 500\n",
      "194 / 500\n",
      "195 / 500\n",
      "196 / 500\n",
      "197 / 500\n",
      "198 / 500\n",
      "199 / 500\n",
      "200 / 500\n",
      "201 / 500\n",
      "202 / 500\n",
      "203 / 500\n",
      "204 / 500\n",
      "205 / 500\n",
      "206 / 500\n",
      "207 / 500\n",
      "208 / 500\n",
      "209 / 500\n",
      "210 / 500\n",
      "211 / 500\n",
      "212 / 500\n",
      "213 / 500\n",
      "214 / 500\n",
      "215 / 500\n",
      "216 / 500\n",
      "217 / 500\n",
      "218 / 500\n",
      "219 / 500\n",
      "220 / 500\n",
      "221 / 500\n",
      "222 / 500\n",
      "223 / 500\n",
      "224 / 500\n",
      "225 / 500\n",
      "226 / 500\n",
      "227 / 500\n",
      "228 / 500\n",
      "229 / 500\n",
      "230 / 500\n",
      "231 / 500\n",
      "232 / 500\n",
      "233 / 500\n",
      "234 / 500\n",
      "235 / 500\n",
      "236 / 500\n",
      "237 / 500\n",
      "238 / 500\n",
      "239 / 500\n",
      "240 / 500\n",
      "241 / 500\n",
      "242 / 500\n",
      "243 / 500\n",
      "244 / 500\n",
      "245 / 500\n",
      "246 / 500\n",
      "247 / 500\n",
      "248 / 500\n",
      "249 / 500\n",
      "250 / 500\n",
      "251 / 500\n",
      "252 / 500\n",
      "253 / 500\n",
      "254 / 500\n",
      "255 / 500\n",
      "256 / 500\n",
      "257 / 500\n",
      "258 / 500\n",
      "259 / 500\n",
      "260 / 500\n",
      "261 / 500\n",
      "262 / 500\n",
      "263 / 500\n",
      "264 / 500\n",
      "265 / 500\n",
      "266 / 500\n",
      "267 / 500\n",
      "268 / 500\n",
      "269 / 500\n",
      "270 / 500\n",
      "271 / 500\n",
      "272 / 500\n",
      "273 / 500\n",
      "274 / 500\n",
      "275 / 500\n",
      "276 / 500\n",
      "277 / 500\n",
      "278 / 500\n",
      "279 / 500\n",
      "280 / 500\n",
      "281 / 500\n",
      "282 / 500\n",
      "283 / 500\n",
      "284 / 500\n",
      "285 / 500\n",
      "286 / 500\n",
      "287 / 500\n",
      "288 / 500\n",
      "289 / 500\n",
      "290 / 500\n",
      "291 / 500\n",
      "292 / 500\n",
      "293 / 500\n",
      "294 / 500\n",
      "295 / 500\n",
      "296 / 500\n",
      "297 / 500\n",
      "298 / 500\n",
      "299 / 500\n",
      "300 / 500\n",
      "301 / 500\n",
      "302 / 500\n",
      "303 / 500\n",
      "304 / 500\n",
      "305 / 500\n",
      "306 / 500\n",
      "307 / 500\n",
      "308 / 500\n",
      "309 / 500\n",
      "310 / 500\n",
      "311 / 500\n",
      "312 / 500\n",
      "313 / 500\n",
      "314 / 500\n",
      "315 / 500\n",
      "316 / 500\n",
      "317 / 500\n",
      "318 / 500\n",
      "319 / 500\n",
      "320 / 500\n",
      "321 / 500\n",
      "322 / 500\n",
      "323 / 500\n",
      "324 / 500\n",
      "325 / 500\n",
      "326 / 500\n",
      "327 / 500\n",
      "328 / 500\n",
      "329 / 500\n",
      "330 / 500\n",
      "331 / 500\n",
      "332 / 500\n",
      "333 / 500\n",
      "334 / 500\n",
      "335 / 500\n",
      "336 / 500\n",
      "337 / 500\n",
      "338 / 500\n",
      "339 / 500\n",
      "340 / 500\n",
      "341 / 500\n",
      "342 / 500\n",
      "343 / 500\n",
      "344 / 500\n",
      "345 / 500\n",
      "346 / 500\n",
      "347 / 500\n",
      "348 / 500\n",
      "349 / 500\n",
      "350 / 500\n",
      "351 / 500\n",
      "352 / 500\n",
      "353 / 500\n",
      "354 / 500\n",
      "355 / 500\n",
      "356 / 500\n",
      "357 / 500\n",
      "358 / 500\n",
      "359 / 500\n",
      "360 / 500\n",
      "361 / 500\n",
      "362 / 500\n",
      "363 / 500\n",
      "364 / 500\n",
      "365 / 500\n",
      "366 / 500\n",
      "367 / 500\n",
      "368 / 500\n",
      "369 / 500\n",
      "370 / 500\n",
      "371 / 500\n",
      "372 / 500\n",
      "373 / 500\n",
      "374 / 500\n",
      "375 / 500\n",
      "376 / 500\n",
      "377 / 500\n",
      "378 / 500\n",
      "379 / 500\n",
      "380 / 500\n",
      "381 / 500\n",
      "382 / 500\n",
      "383 / 500\n",
      "384 / 500\n",
      "385 / 500\n",
      "386 / 500\n",
      "387 / 500\n",
      "388 / 500\n",
      "389 / 500\n",
      "390 / 500\n",
      "391 / 500\n",
      "392 / 500\n",
      "393 / 500\n",
      "394 / 500\n",
      "395 / 500\n",
      "396 / 500\n",
      "397 / 500\n",
      "398 / 500\n",
      "399 / 500\n",
      "400 / 500\n",
      "401 / 500\n",
      "402 / 500\n",
      "403 / 500\n",
      "404 / 500\n",
      "405 / 500\n",
      "406 / 500\n",
      "407 / 500\n",
      "408 / 500\n",
      "409 / 500\n",
      "410 / 500\n",
      "411 / 500\n",
      "412 / 500\n",
      "413 / 500\n",
      "414 / 500\n",
      "415 / 500\n",
      "416 / 500\n",
      "417 / 500\n",
      "418 / 500\n",
      "419 / 500\n",
      "420 / 500\n",
      "421 / 500\n",
      "422 / 500\n",
      "423 / 500\n",
      "424 / 500\n",
      "425 / 500\n",
      "426 / 500\n",
      "427 / 500\n",
      "428 / 500\n",
      "429 / 500\n",
      "430 / 500\n",
      "431 / 500\n",
      "432 / 500\n",
      "433 / 500\n",
      "434 / 500\n",
      "435 / 500\n",
      "436 / 500\n",
      "437 / 500\n",
      "438 / 500\n",
      "439 / 500\n",
      "440 / 500\n",
      "441 / 500\n",
      "442 / 500\n",
      "443 / 500\n",
      "444 / 500\n",
      "445 / 500\n",
      "446 / 500\n",
      "447 / 500\n",
      "448 / 500\n",
      "449 / 500\n",
      "450 / 500\n",
      "451 / 500\n",
      "452 / 500\n",
      "453 / 500\n",
      "454 / 500\n",
      "455 / 500\n",
      "456 / 500\n",
      "457 / 500\n",
      "458 / 500\n",
      "459 / 500\n",
      "460 / 500\n",
      "461 / 500\n",
      "462 / 500\n",
      "463 / 500\n",
      "464 / 500\n",
      "465 / 500\n",
      "466 / 500\n",
      "467 / 500\n",
      "468 / 500\n",
      "469 / 500\n",
      "470 / 500\n",
      "471 / 500\n",
      "472 / 500\n",
      "473 / 500\n",
      "474 / 500\n",
      "475 / 500\n",
      "476 / 500\n",
      "477 / 500\n",
      "478 / 500\n",
      "479 / 500\n",
      "480 / 500\n",
      "481 / 500\n",
      "482 / 500\n",
      "483 / 500\n",
      "484 / 500\n",
      "485 / 500\n",
      "486 / 500\n",
      "487 / 500\n",
      "488 / 500\n",
      "489 / 500\n",
      "490 / 500\n",
      "491 / 500\n",
      "492 / 500\n",
      "493 / 500\n",
      "494 / 500\n",
      "495 / 500\n",
      "496 / 500\n",
      "497 / 500\n",
      "498 / 500\n",
      "499 / 500\n"
     ]
    }
   ],
   "source": [
    "mean_similarities = [[] for i in range(0,13)]\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        test_inputs = batch[0]\n",
    "        outputs = model(test_inputs)\n",
    "        hidden_states = outputs[2]  # The last hidden-state is the first element of the output tuple\n",
    "        for i, hidden_state in enumerate(hidden_states):\n",
    "            for j in range(0,batch_size):\n",
    "                hidden_state_np = hidden_state[j].cpu().numpy()\n",
    "                contextual_similarity = cosine_similarity(hidden_state_np,hidden_state_np)\n",
    "                mean_similarities[i].append(np.mean(contextual_similarity))\n",
    "        print(step,\"/\",len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_similarities_np = np.array(mean_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29659212, 0.4286276 , 0.55409384, ..., 0.2627282 , 0.23423457,\n",
       "        0.31067103],\n",
       "       [0.54460746, 0.6707187 , 0.7756945 , ..., 0.49183565, 0.45353505,\n",
       "        0.5507088 ],\n",
       "       [0.54024374, 0.6595992 , 0.75318027, ..., 0.49116823, 0.45554706,\n",
       "        0.53942394],\n",
       "       ...,\n",
       "       [0.56883013, 0.6455235 , 0.74219525, ..., 0.5620757 , 0.46991804,\n",
       "        0.5875374 ],\n",
       "       [0.66438204, 0.7242289 , 0.8074277 , ..., 0.6553541 , 0.5167835 ,\n",
       "        0.6864211 ],\n",
       "       [0.9933552 , 0.9927799 , 0.9973526 , ..., 0.98054695, 0.76677585,\n",
       "        0.8873472 ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_similarities_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38357255, 0.6140497 , 0.6038382 , 0.5596043 , 0.5631125 ,\n",
       "       0.5586654 , 0.5246606 , 0.5254687 , 0.5379017 , 0.56307393,\n",
       "       0.6253561 , 0.71307486, 0.96989065], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_similarities_np,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
