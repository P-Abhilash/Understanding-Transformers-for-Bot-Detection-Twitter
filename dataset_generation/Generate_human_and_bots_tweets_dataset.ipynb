{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Hbg8VoPLn6o"
   },
   "source": [
    "# Connect to Google Drive to access the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "CWDHUbL8LSRu",
    "outputId": "321f5fea-16a9-4b8a-96ec-52bd2a057c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbIuYD-0LUMN"
   },
   "source": [
    "## Read and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3a1QlvEMLR7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evU6AP0TMSIP"
   },
   "outputs": [],
   "source": [
    "dataset_file_tweets_humans=\"/drive/My Drive/app/data/evaluation/tweets-humans.csv\"\n",
    "dataset_file_tweets_bots=\"/drive/My Drive/app/data/evaluation/tweets-bots.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJWr_rBVLWvY"
   },
   "outputs": [],
   "source": [
    "with open(dataset_file_tweets_humans, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
    "  dataset_tweets_humans = pd.read_csv(file)\n",
    "  \n",
    "dataset_tweets_humans['label'] = 0\n",
    "\n",
    "with open(dataset_file_tweets_bots, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
    "  dataset_tweets_bots = pd.read_csv(file)\n",
    "  \n",
    "dataset_tweets_bots['label'] = 1\n",
    "\n",
    "dataset_complete = dataset_tweets_humans.append(dataset_tweets_bots,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7ZEcJPnNajs"
   },
   "source": [
    "This dataset contains users that appear in both categories, so we delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pz5N9mACMFzg"
   },
   "outputs": [],
   "source": [
    "intersect = np.intersect1d(dataset_tweets_humans.groupby('user').size().index.values,dataset_tweets_bots.groupby('user').size().index.values)\n",
    "\n",
    "for user in intersect:\n",
    "  dataset_complete = dataset_complete[dataset_complete['user'] != user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEOiRH1rOqRk"
   },
   "source": [
    "Now we are going to take a random sample of 500000 tweets for training and 100000 tweets for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu05eLClOool"
   },
   "outputs": [],
   "source": [
    "# Take a random sample of 500000 tweets for training set\n",
    "dataset_train = dataset_complete.sample(n=500000)\n",
    "\n",
    "# tweets not in training set\n",
    "dataset_complement = dataset_complete.loc[dataset_complete.index.difference(dataset_train.index),:]\n",
    "\n",
    "# take a random sample of 100k in the complementary set\n",
    "dataset_test = dataset_complement.sample(n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpP3YPAkQvHn"
   },
   "source": [
    "We can save our training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buRiLDNsQK1z"
   },
   "outputs": [],
   "source": [
    "dataset_train.to_csv('/drive/My Drive/app/tweets-humans-and-bots-train.csv')\n",
    "dataset_test.to_csv('/drive/My Drive/app/tweets-humans-and-bots-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S21Aymr8SGWg"
   },
   "source": [
    "Now we have generated our training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IH-q2h1u2t5A"
   },
   "source": [
    "# Read and prepare the preprocessed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaFyccgh2-zM"
   },
   "source": [
    "If you want to create the same dataset, but with preprocessed tweets, we can use the dataset_train and dataset_test indexes, but using the complete dataset of preprocessed tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7ambvSB2jNL"
   },
   "outputs": [],
   "source": [
    "dataset_file_tweets_humans=\"/drive/My Drive/app/data/evaluation/tweets-humans-preprocessed.csv\"\n",
    "dataset_file_tweets_bots=\"/drive/My Drive/app/data/evaluation/tweets-bots-preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neTNqpb82jNy"
   },
   "outputs": [],
   "source": [
    "with open(dataset_file_tweets_humans, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
    "  dataset_tweets_humans = pd.read_csv(file)\n",
    "  \n",
    "dataset_tweets_humans['label'] = 0\n",
    "\n",
    "with open(dataset_file_tweets_bots, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
    "  dataset_tweets_bots = pd.read_csv(file)\n",
    "  \n",
    "dataset_tweets_bots['label'] = 1\n",
    "\n",
    "dataset_complete = dataset_tweets_humans.append(dataset_tweets_bots,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IB9rUkJuzgoG"
   },
   "outputs": [],
   "source": [
    "dataset_train_preprocessed = dataset_complete.loc[dataset_train.index]\n",
    "dataset_test_preprocessed = dataset_complete.loc[dataset_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tje-azzQ4yUb"
   },
   "outputs": [],
   "source": [
    "dataset_train_preprocessed.to_csv('/drive/My Drive/app/tweets-humans-and-bots-preprocessed-train.csv')\n",
    "dataset_test_preprocessed.to_csv('/drive/My Drive/app/tweets-humans-and-bots-preprocessed-test.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Generate human and bots tweets dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
