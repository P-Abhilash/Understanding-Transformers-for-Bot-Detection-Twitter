{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, AdamW, BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/CoLA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(directory+\"train.tsv\", sep = \"\\t\", header=None)\n",
    "test = pd.read_csv(directory+\"test.tsv\", sep = \"\\t\")\n",
    "dev = pd.read_csv(directory+\"dev.tsv\", sep = \"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"BERT\":\n",
    "    train_sentences = train[3].values\n",
    "    #train_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in train_sentences]\n",
    "    train_labels = train[1].values\n",
    "\n",
    "    test_sentences = dev[3].values\n",
    "    #test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
    "    test_labels = dev[1].values\n",
    "    \n",
    "elif model_name == \"gpt2\":\n",
    "    train_sentences = train[3].values\n",
    "    #train_sentences = [\"_start_ \" + sentence + \" _classify_\" for sentence in train_sentences]\n",
    "    train_labels = train[1].values\n",
    "    \n",
    "    test_sentences = dev[3].values\n",
    "    #test_sentences = [\"_start_ \" + sentence + \" _classify_\" for sentence in test_sentences]\n",
    "    test_labels = dev[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "if model_name == \"BERT\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "elif model_name == \"gpt2\":\n",
    "    special_tokens_dict = {'cls_token': '_classify_','bos_token': '_start_'}\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', do_lower_case=True)\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = [tokenizer.encode(sent) for sent in train_sentences]\n",
    "test_input_ids = [tokenizer.encode(sent) for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tokenizer.convert_tokens_to_ids(\"_start_\")\n",
    "classify = tokenizer.convert_tokens_to_ids(\"_classify_\")\n",
    "train_input_ids = [[start] + tokenizer.encode(sent)[:(max_length-2)] + [classify] for sent in train_sentences]\n",
    "test_input_ids = [[start] + tokenizer.encode(sent)[:(max_length-2)] + [classify] for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_position_train = np.argmax(train_input_ids==classify,axis=1)\n",
    "cls_position_test = np.argmax(test_input_ids==classify,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attention_masks = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for seq in train_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    train_attention_masks.append(seq_mask)\n",
    "    \n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_input_ids)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_cls_positions= torch.tensor(cls_position_train)\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_cls_positions=torch.tensor(cls_position_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_cls_positions)\n",
    "#train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_cls_positions)\n",
    "#test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'transformers.modeling_gpt2.GPT2DoubleHeadsModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'transformers.modeling_gpt2.GPT2Model' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.normalization.LayerNorm' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'transformers.modeling_utils.SequenceSummary' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/test/anaconda3/lib/python3.6/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Identity' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"BERT\":\n",
    "    #model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model = torch.load('../models/COLA-finetuned.pt')\n",
    "elif model_name == \"gpt2\":\n",
    "    #model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\", num_labels = 2)\n",
    "    model = torch.load('../models/COLA-finetuned-GPT2.pt')\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linears = [nn.Linear(768, 2).to(torch.device(\"cuda:0\")) for  x in range(0,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = [list(linears[i].named_parameters()) for i in range(0,13)]\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [[\n",
    "    {'params': [p for n, p in param_optimizer[i] if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer[i] if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "] for i in range(0,13)]\n",
    "\n",
    "optimizer = [AdamW(optimizer_grouped_parameters[i],\n",
    "                     lr=2e-5,) for i in range(0,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name==\"BERT\":\n",
    "  model.bert.config.output_hidden_states = True\n",
    "  model.bert.config.is_decoder = False\n",
    "  model.bert.encoder.output_hidden_states = True\n",
    "  for i in range(0,len(model.bert.encoder.layer)): \n",
    "    model.bert.encoder.layer[i].is_decoder = False\n",
    "    model.bert.encoder.layer[i].output_hidden_states = True\n",
    "else:\n",
    "    model.transformer.output_hidden_states = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [22:14<00:00, 444.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_sets = [[] for i in range(0,13)]\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  for linear in linears:\n",
    "    linear.train()\n",
    "    \n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_masks, b_labels, b_cls_positions = batch\n",
    "    #b_input_ids, b_masks, b_labels = batch\n",
    "    for i in range(0,13):\n",
    "      optimizer[i].zero_grad()\n",
    "    if(model_name==\"gpt2\"):\n",
    "        #model.transformer.output_hidden_states = True\n",
    "        outputs = model.transformer(b_input_ids, attention_mask=b_masks)\n",
    "        logits = []\n",
    "        for i in range(0,13):\n",
    "            hl = outputs[2][i] ## We taken the all hidden states and take the l layer\n",
    "            #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "            #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "            logits.append(linears[i](hl[range(0,len(b_cls_positions)),b_cls_positions])) ## We take the classification token embedding to train the linear layer\n",
    "    else:\n",
    "        #h0 = model.bert.embeddings(b_input_ids)\n",
    "        #logits = linear(h0.view(-1,98304))\n",
    "        outputs = model.bert(b_input_ids, attention_mask=b_masks)\n",
    "        logits = []\n",
    "        for i in range(0,13):\n",
    "            hl = outputs[2][i] ## We taken the all hidden states and take the l layer\n",
    "            #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "            #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "            logits.append(linears[i](hl[:,0])) ## We take the first token [CLS] embedding to train the linear layer\n",
    "\n",
    "    for i in range(0,13): \n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      loss=loss_fct(logits[i].view(-1, logits[i].size(-1)),\n",
    "                              b_labels.view(-1))\n",
    "      \n",
    "      train_loss_sets[i].append(loss.item())\n",
    "      loss.backward(retain_graph=True)\n",
    "      optimizer[i].step()\n",
    "    \n",
    "#if model == \"gpt2\":\n",
    "#  torch.save(linear, \"/content/drive/My Drive/linear_gpt2_layer1.pt\")\n",
    "#else:\n",
    "#  torch.save(linear,\"/content/drive/My Drive/linear_BERT.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [[] for i in range(0,13)]\n",
    "\n",
    "for linear in linears:\n",
    "    linear.eval()\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_masks, b_labels, b_cls_positions = batch\n",
    "    #b_input_ids, b_masks, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        if(model_name==\"gpt2\"):\n",
    "            outputs = model.transformer(b_input_ids, attention_mask=b_masks)\n",
    "            logits = []\n",
    "            for i in range(0,13):\n",
    "                hl = outputs[2][i] ## We taken the all hidden states and take the l layer\n",
    "                #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "                #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "                logits.append(linears[i](hl[range(0,len(b_cls_positions)),b_cls_positions])) ## We take the classification token embedding to t\n",
    "        else:\n",
    "            outputs = outputs = model.bert(b_input_ids, attention_mask=b_masks)\n",
    "            logits = []\n",
    "            for j in range(0,13):\n",
    "                hl = outputs[2][j] ## We taken the all hidden states and take the l layer\n",
    "                #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "                #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "                logits.append(linears[j](hl[:,0])) ## We take the first token [CLS] embedding to train the linear layer\n",
    "    for j in range(0,13):\n",
    "        logits[j] = logits[j].detach().cpu().numpy()\n",
    "        preds[j].append(logits[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       322\n",
      "           1     0.6913    1.0000    0.8175       721\n",
      "\n",
      "    accuracy                         0.6913      1043\n",
      "   macro avg     0.3456    0.5000    0.4087      1043\n",
      "weighted avg     0.4779    0.6913    0.5651      1043\n",
      "\n",
      "Accuracy:  0.6912751677852349\n",
      "layer 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       322\n",
      "           1     0.6913    1.0000    0.8175       721\n",
      "\n",
      "    accuracy                         0.6913      1043\n",
      "   macro avg     0.3456    0.5000    0.4087      1043\n",
      "weighted avg     0.4779    0.6913    0.5651      1043\n",
      "\n",
      "Accuracy:  0.6912751677852349\n",
      "layer 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       322\n",
      "           1     0.6910    0.9986    0.8168       721\n",
      "\n",
      "    accuracy                         0.6903      1043\n",
      "   macro avg     0.3455    0.4993    0.4084      1043\n",
      "weighted avg     0.4777    0.6903    0.5646      1043\n",
      "\n",
      "Accuracy:  0.6903163950143816\n",
      "layer 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.0248    0.0473       322\n",
      "           1     0.6943    0.9889    0.8158       721\n",
      "\n",
      "    accuracy                         0.6913      1043\n",
      "   macro avg     0.5971    0.5069    0.4316      1043\n",
      "weighted avg     0.6343    0.6913    0.5785      1043\n",
      "\n",
      "Accuracy:  0.6912751677852349\n",
      "layer 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3000    0.0186    0.0351       322\n",
      "           1     0.6911    0.9806    0.8108       721\n",
      "\n",
      "    accuracy                         0.6836      1043\n",
      "   macro avg     0.4956    0.4996    0.4229      1043\n",
      "weighted avg     0.5704    0.6836    0.5713      1043\n",
      "\n",
      "Accuracy:  0.6836049856184084\n",
      "layer 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4474    0.0528    0.0944       322\n",
      "           1     0.6965    0.9709    0.8111       721\n",
      "\n",
      "    accuracy                         0.6874      1043\n",
      "   macro avg     0.5719    0.5118    0.4528      1043\n",
      "weighted avg     0.6196    0.6874    0.5899      1043\n",
      "\n",
      "Accuracy:  0.6874400767018217\n",
      "layer 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3438    0.0683    0.1140       322\n",
      "           1     0.6936    0.9417    0.7988       721\n",
      "\n",
      "    accuracy                         0.6721      1043\n",
      "   macro avg     0.5187    0.5050    0.4564      1043\n",
      "weighted avg     0.5856    0.6721    0.5874      1043\n",
      "\n",
      "Accuracy:  0.6720997123681688\n",
      "layer 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.4596    0.5095       322\n",
      "           1     0.7781    0.8460    0.8106       721\n",
      "\n",
      "    accuracy                         0.7267      1043\n",
      "   macro avg     0.6747    0.6528    0.6600      1043\n",
      "weighted avg     0.7143    0.7267    0.7177      1043\n",
      "\n",
      "Accuracy:  0.7267497603068073\n",
      "layer 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6139    0.5776    0.5952       322\n",
      "           1     0.8162    0.8377    0.8268       721\n",
      "\n",
      "    accuracy                         0.7574      1043\n",
      "   macro avg     0.7150    0.7077    0.7110      1043\n",
      "weighted avg     0.7537    0.7574    0.7553      1043\n",
      "\n",
      "Accuracy:  0.7574304889741131\n",
      "layer 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6036    0.5248    0.5615       322\n",
      "           1     0.7995    0.8460    0.8221       721\n",
      "\n",
      "    accuracy                         0.7469      1043\n",
      "   macro avg     0.7015    0.6854    0.6918      1043\n",
      "weighted avg     0.7390    0.7469    0.7416      1043\n",
      "\n",
      "Accuracy:  0.7468839884947267\n",
      "layer 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5772    0.5807    0.5789       322\n",
      "           1     0.8122    0.8100    0.8111       721\n",
      "\n",
      "    accuracy                         0.7392      1043\n",
      "   macro avg     0.6947    0.6954    0.6950      1043\n",
      "weighted avg     0.7397    0.7392    0.7394      1043\n",
      "\n",
      "Accuracy:  0.7392138063279002\n",
      "layer 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5908    0.5559    0.5728       322\n",
      "           1     0.8068    0.8280    0.8172       721\n",
      "\n",
      "    accuracy                         0.7440      1043\n",
      "   macro avg     0.6988    0.6920    0.6950      1043\n",
      "weighted avg     0.7401    0.7440    0.7418      1043\n",
      "\n",
      "Accuracy:  0.7440076701821668\n",
      "layer 12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6620    0.4379    0.5271       322\n",
      "           1     0.7819    0.9001    0.8369       721\n",
      "\n",
      "    accuracy                         0.7574      1043\n",
      "   macro avg     0.7219    0.6690    0.6820      1043\n",
      "weighted avg     0.7449    0.7574    0.7412      1043\n",
      "\n",
      "Accuracy:  0.7574304889741131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "for i in range(0,13):\n",
    "    print(\"layer \" + str(i))\n",
    "    predictions = []\n",
    "    for pred in preds[i]:\n",
    "        p = np.argmax(pred, axis = 1)\n",
    "        for label in p:\n",
    "            predictions.append(label)\n",
    "    print(classification_report(test_labels, predictions, digits = 4))\n",
    "    print(\"Accuracy: \", accuracy_score(test_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
