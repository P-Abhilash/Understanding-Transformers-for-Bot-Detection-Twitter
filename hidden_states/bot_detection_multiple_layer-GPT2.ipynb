{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from transformers import GPT2DoubleHeadsModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, BertTokenizer\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = \"../data/bot_detection\"\n",
    "train = pd.read_csv(directory + \"train.csv\", header=None)\n",
    "test = pd.read_csv(directory + \"test.csv\", header=None)\n",
    "\n",
    "train = pd.DataFrame({\n",
    "    'id':range(len(train)),\n",
    "    'label':train[0],\n",
    "    'mark':['a']*train.shape[0],\n",
    "    'text': train[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'id':range(len(test)),\n",
    "    'label':test[0],\n",
    "    'mark':['a']*test.shape[0],\n",
    "    'text': test[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "train.columns = [\"index\", \"label\", \"mark\", \"tweet\"]\n",
    "test.columns =  [\"index\", \"label\", \"mark\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "if model_name == \"BERT\":\n",
    "    train_sentences = train.tweet.values\n",
    "    train_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in train_sentences]\n",
    "    train_labels = train.label.values\n",
    "\n",
    "    test_sentences = test.tweet.values\n",
    "    test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
    "    test_labels = test.label.values\n",
    "\n",
    "elif model_name == \"gpt2\":\n",
    "    train_sentences = train.tweet.values\n",
    "    train_labels = train.label.values\n",
    "    test_sentences = test.tweet.values\n",
    "    test_labels = test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "if model_name == \"BERT\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "elif model_name == \"gpt2\":\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', do_lower_case=True)\n",
    "    \n",
    "train_input_ids = [tokenizer.encode(sent) for sent in train_sentences]\n",
    "test_input_ids = [tokenizer.encode(sent) for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 128\n",
    "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_input_ids)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "batch_size = 8\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"BERT\":\n",
    "    model = torch.load('../models/BERT_Classifier_Large.pt')\n",
    "elif model_name == \"gpt2\":\n",
    "    model = torch.load(\"../models/Gpt2_Classifier_Large.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linears = [nn.Linear(768, 2).to(torch.device(\"cuda:0\")) for  x in range(0,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = [list(linears[i].named_parameters()) for i in range(0,12)]\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [[\n",
    "    {'params': [p for n, p in param_optimizer[i] if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer[i] if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "] for i in range(0,12)]\n",
    "\n",
    "optimizer = [AdamW(optimizer_grouped_parameters[i],\n",
    "                     lr=2e-5,) for i in range(0,12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name==\"BERT\":\n",
    "  model.bert.config.output_hidden_states = True\n",
    "  model.bert.config.is_decoder = False\n",
    "  model.bert.encoder.output_hidden_states = True\n",
    "  for i in range(0,len(model.bert.encoder.layer)): \n",
    "    model.bert.encoder.layer[i].is_decoder = False\n",
    "    model.bert.encoder.layer[i].output_hidden_states = True\n",
    "else:\n",
    "    model.transformer.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple layers training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_sets = [[] for i in range(0,12)]\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  for linear in linears:\n",
    "    linear.train()\n",
    "    \n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_labels = batch\n",
    "    for i in range(0,12):\n",
    "      optimizer[i].zero_grad()\n",
    "    if(model_name==\"gpt2\"):\n",
    "        #model.transformer.output_hidden_states = True\n",
    "        outputs = model.transformer(b_input_ids)\n",
    "        logits = []\n",
    "        for i in range(0,12):\n",
    "            hl = outputs[2][i+1] ## We taken the all hidden states and take the l layer\n",
    "            #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "            #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "            logits.append(linears[i](hl[:,-1])) ## We take the last token embedding to train the linear layer\n",
    "    else:\n",
    "        #h0 = model.bert.embeddings(b_input_ids)\n",
    "        #logits = linear(h0.view(-1,98304))\n",
    "        outputs = model.bert(b_input_ids)\n",
    "        logits = []\n",
    "        for i in range(0,12):\n",
    "            hl = outputs[2][i+1] ## We taken the all hidden states and take the l layer\n",
    "            #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "            #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "            logits.append(linears[i](hl[:,0])) ## We take the first token [CLS] embedding to train the linear layer\n",
    "\n",
    "    for i in range(0,12): \n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      loss=loss_fct(logits[i].view(-1, logits[i].size(-1)),\n",
    "                              b_labels.view(-1))\n",
    "      \n",
    "      train_loss_sets[i].append(loss.item())\n",
    "      loss.backward(retain_graph=True)\n",
    "      optimizer[i].step()\n",
    "    \n",
    "#if model == \"gpt2\":\n",
    "#  torch.save(linear, \"/content/drive/My Drive/linear_gpt2_layer1.pt\")\n",
    "#else:\n",
    "#  torch.save(linear,\"/content/drive/My Drive/linear_BERT.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  0.0 %\n",
      "Processing:  1.0 %\n",
      "Processing:  2.0 %\n",
      "Processing:  3.0 %\n",
      "Processing:  4.0 %\n",
      "Processing:  5.0 %\n",
      "Processing:  6.0 %\n",
      "Processing:  7.0 %\n",
      "Processing:  8.0 %\n",
      "Processing:  9.0 %\n",
      "Processing:  10.0 %\n",
      "Processing:  11.0 %\n",
      "Processing:  12.0 %\n",
      "Processing:  13.0 %\n",
      "Processing:  14.0 %\n",
      "Processing:  15.0 %\n",
      "Processing:  16.0 %\n",
      "Processing:  17.0 %\n",
      "Processing:  18.0 %\n",
      "Processing:  19.0 %\n",
      "Processing:  20.0 %\n",
      "Processing:  21.0 %\n",
      "Processing:  22.0 %\n",
      "Processing:  23.0 %\n",
      "Processing:  24.0 %\n",
      "Processing:  25.0 %\n",
      "Processing:  26.0 %\n",
      "Processing:  27.0 %\n",
      "Processing:  28.0 %\n",
      "Processing:  29.0 %\n",
      "Processing:  30.0 %\n",
      "Processing:  31.0 %\n",
      "Processing:  32.0 %\n",
      "Processing:  33.0 %\n",
      "Processing:  34.0 %\n",
      "Processing:  35.0 %\n",
      "Processing:  36.0 %\n",
      "Processing:  37.0 %\n",
      "Processing:  38.0 %\n",
      "Processing:  39.0 %\n",
      "Processing:  40.0 %\n",
      "Processing:  41.0 %\n",
      "Processing:  42.0 %\n",
      "Processing:  43.0 %\n",
      "Processing:  44.0 %\n",
      "Processing:  45.0 %\n",
      "Processing:  46.0 %\n",
      "Processing:  47.0 %\n",
      "Processing:  48.0 %\n",
      "Processing:  49.0 %\n",
      "Processing:  50.0 %\n",
      "Processing:  51.0 %\n",
      "Processing:  52.0 %\n",
      "Processing:  53.0 %\n",
      "Processing:  54.0 %\n",
      "Processing:  55.0 %\n",
      "Processing:  56.0 %\n",
      "Processing:  57.0 %\n",
      "Processing:  58.0 %\n",
      "Processing:  59.0 %\n",
      "Processing:  60.0 %\n",
      "Processing:  61.0 %\n",
      "Processing:  62.0 %\n",
      "Processing:  63.0 %\n",
      "Processing:  64.0 %\n",
      "Processing:  65.0 %\n",
      "Processing:  66.0 %\n",
      "Processing:  67.0 %\n",
      "Processing:  68.0 %\n",
      "Processing:  69.0 %\n",
      "Processing:  70.0 %\n",
      "Processing:  71.0 %\n",
      "Processing:  72.0 %\n",
      "Processing:  73.0 %\n",
      "Processing:  74.0 %\n",
      "Processing:  75.0 %\n",
      "Processing:  76.0 %\n",
      "Processing:  77.0 %\n",
      "Processing:  78.0 %\n",
      "Processing:  79.0 %\n",
      "Processing:  80.0 %\n",
      "Processing:  81.0 %\n",
      "Processing:  82.0 %\n",
      "Processing:  83.0 %\n",
      "Processing:  84.0 %\n",
      "Processing:  85.0 %\n",
      "Processing:  86.0 %\n",
      "Processing:  87.0 %\n",
      "Processing:  88.0 %\n",
      "Processing:  89.0 %\n",
      "Processing:  90.0 %\n",
      "Processing:  91.0 %\n",
      "Processing:  92.0 %\n",
      "Processing:  93.0 %\n",
      "Processing:  94.0 %\n",
      "Processing:  95.0 %\n",
      "Processing:  96.0 %\n",
      "Processing:  97.0 %\n",
      "Processing:  98.0 %\n",
      "Processing:  99.0 %\n"
     ]
    }
   ],
   "source": [
    "preds = [[] for i in range(0,12)]\n",
    "\n",
    "for i in range(int(len(test_inputs)/10)):\n",
    "    batch = (test_inputs[i*10: (i+1)*10].to(device))\n",
    "    with torch.no_grad():\n",
    "        if(model_name==\"gpt2\"):\n",
    "            outputs = model.transformer(batch)\n",
    "            logits = []\n",
    "            for j in range(0,12):\n",
    "                hl = outputs[2][j+1] ## We taken the all hidden states and take the l layer\n",
    "                #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "                #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "                logits.append(linears[j](hl[:,-1])) ## We take the last token embedding to train the linear layer\n",
    "        else:\n",
    "            outputs = outputs = model.bert(batch)\n",
    "            logits = []\n",
    "            for j in range(0,12):\n",
    "                hl = outputs[2][j+1] ## We taken the all hidden states and take the l layer\n",
    "                #h0 = outputs[3][0] ## Here we are using embeddings layer \n",
    "                #logits = linear(h0.view(-1,98304)) ## We flat the embeddings\n",
    "                logits.append(linears[j](hl[:,0])) ## We take the first token [CLS] embedding to train the linear layer\n",
    "    for j in range(0,12):\n",
    "        logits[j] = logits[j].detach().cpu().numpy()\n",
    "        preds[j].append(logits[j])\n",
    "    if i%100 == 0:\n",
    "        print(\"Processing: \", i*10/1000, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.6965    0.7777    0.7349     55712\n",
      "          1     0.6723    0.5737    0.6191     44288\n",
      "\n",
      "avg / total     0.6858    0.6874    0.6836    100000\n",
      "\n",
      "Accuracy:  0.68737\n",
      "layer 2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7277    0.8014    0.7627     55712\n",
      "          1     0.7136    0.6227    0.6651     44288\n",
      "\n",
      "avg / total     0.7215    0.7222    0.7195    100000\n",
      "\n",
      "Accuracy:  0.72225\n",
      "layer 3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7252    0.8233    0.7711     55712\n",
      "          1     0.7321    0.6075    0.6640     44288\n",
      "\n",
      "avg / total     0.7282    0.7277    0.7237    100000\n",
      "\n",
      "Accuracy:  0.72772\n",
      "layer 4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7273    0.8301    0.7753     55712\n",
      "          1     0.7401    0.6084    0.6678     44288\n",
      "\n",
      "avg / total     0.7329    0.7319    0.7277    100000\n",
      "\n",
      "Accuracy:  0.73193\n",
      "layer 5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7314    0.8490    0.7858     55712\n",
      "          1     0.7619    0.6078    0.6762     44288\n",
      "\n",
      "avg / total     0.7449    0.7422    0.7373    100000\n",
      "\n",
      "Accuracy:  0.74219\n",
      "layer 6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7634    0.8706    0.8135     55712\n",
      "          1     0.8023    0.6606    0.7246     44288\n",
      "\n",
      "avg / total     0.7807    0.7776    0.7741    100000\n",
      "\n",
      "Accuracy:  0.77761\n",
      "layer 7\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7856    0.8819    0.8310     55712\n",
      "          1     0.8243    0.6973    0.7555     44288\n",
      "\n",
      "avg / total     0.8028    0.8001    0.7976    100000\n",
      "\n",
      "Accuracy:  0.80013\n",
      "layer 8\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7893    0.8887    0.8361     55712\n",
      "          1     0.8337    0.7015    0.7619     44288\n",
      "\n",
      "avg / total     0.8090    0.8058    0.8032    100000\n",
      "\n",
      "Accuracy:  0.80584\n",
      "layer 9\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8052    0.8843    0.8429     55712\n",
      "          1     0.8339    0.7309    0.7790     44288\n",
      "\n",
      "avg / total     0.8179    0.8164    0.8146    100000\n",
      "\n",
      "Accuracy:  0.81637\n",
      "layer 10\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8039    0.9088    0.8531     55712\n",
      "          1     0.8627    0.7212    0.7856     44288\n",
      "\n",
      "avg / total     0.8300    0.8257    0.8232    100000\n",
      "\n",
      "Accuracy:  0.82568\n",
      "layer 11\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8102    0.9110    0.8577     55712\n",
      "          1     0.8673    0.7316    0.7937     44288\n",
      "\n",
      "avg / total     0.8355    0.8316    0.8293    100000\n",
      "\n",
      "Accuracy:  0.83155\n",
      "layer 12\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8136    0.9157    0.8616     55712\n",
      "          1     0.8740    0.7362    0.7992     44288\n",
      "\n",
      "avg / total     0.8404    0.8362    0.8340    100000\n",
      "\n",
      "Accuracy:  0.83616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "for i in range(0,12):\n",
    "    print(\"layer \" + str(i+1))\n",
    "    predictions = []\n",
    "    for pred in preds[i]:\n",
    "        p = np.argmax(pred, axis = 1)\n",
    "        for label in p:\n",
    "            predictions.append(label)\n",
    "    print(classification_report(np.asarray(test[\"label\"][:len(predictions)]), predictions, digits = 4))\n",
    "    print(\"Accuracy: \", accuracy_score(np.asarray(test[\"label\"][:len(predictions)]), predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
