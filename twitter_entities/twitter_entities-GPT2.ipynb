{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from torch.utils import data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = []\n",
    "with open(\"../data/bot_detection/train-tweets-POS.txt\") as file:\n",
    "  data_file = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['@','#','U']\n",
    "sentences = []\n",
    "sentence_number = 0\n",
    "sentences.append([])\n",
    "for i,line in enumerate(data_file):\n",
    "    if line!='\\n':\n",
    "        word,POS,prob = line.split('\\t')\n",
    "        POS = POS if (POS in special_tokens) else 'X'\n",
    "        sentences[sentence_number].append((word,POS))\n",
    "    else:\n",
    "        sentence_number = sentence_number + 1\n",
    "        sentences.append([])\n",
    "del sentences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(set(word_pos[1] for sent in sentences for word_pos in sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U,#,@,X'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"<pad>\"] + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"gpt2\":\n",
    "    special_tokens_dict = {'cls_token': '_classify_','bos_token': '_start_', 'pad_token': '<pad>'}\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', do_lower_case=True)\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDataset(data.Dataset):\n",
    "    def __init__(self, tagged_sents):\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for sent in tagged_sents:\n",
    "            words = [word_pos[0] for word_pos in sent]\n",
    "            tags = [word_pos[1] for word_pos in sent]\n",
    "            if model_name == \"gpt2\":\n",
    "                sents.append([\"_start_\"] + words + [\"_classify_\"])\n",
    "                tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
    "            else:\n",
    "                sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "                tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            if(len(tokens)==0): \n",
    "                tokens=[w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "        if(not len(x)==len(y)==len(is_heads)):\n",
    "            print(words, tags)\n",
    "            print(x,y,is_heads)\n",
    "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PosDataset(sentences)\n",
    "data_iter = data.DataLoader(dataset=dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"gpt2\":\n",
    "    #model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "    model = torch.load(\"../models/Gpt2_Classifier_Large.pt\")\n",
    "    model = model.transformer\n",
    "    model.output_hidden_states = True\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "all_nmi =[]\n",
    "for n in range(0,10):\n",
    "    Tags, Embedds = [], [[] for i in range(0,13)]\n",
    "    #Tags, Embedds = [], []\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        _, x, is_heads_b, _, y_b, _ = batch\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            output=model(x)\n",
    "            embedds_b = output[2]\n",
    "            #embedds_b = output[0] # We want to test output embeddings\n",
    "            for i in range(0,13):\n",
    "                for embedds, is_heads, ys in zip(embedds_b[i].cpu().numpy(), is_heads_b, y_b):\n",
    "                    embbed = [hat for head, hat in zip(is_heads, embedds) if head == 1]   \n",
    "                    y = [hat.item() for head, hat in zip(is_heads, ys) if head == 1]  \n",
    "                    Embedds[i].extend(embbed[1:-1])\n",
    "                    if(i==0):\n",
    "                        Tags.extend(y[1:-1])\n",
    "            #for embedds, is_heads, ys in zip(embedds_b.cpu().numpy(), is_heads_b, y_b):\n",
    "                #embbed = [hat for head, hat in zip(is_heads, embedds) if head == 1]   \n",
    "                #y = [hat.item() for head, hat in zip(is_heads, ys) if head == 1]  \n",
    "                #Embedds.extend(embbed[1:-1])\n",
    "                #if(i==0):\n",
    "                    #Tags.extend(y[1:-1])\n",
    "                #Tags.extend(y[1:-1])\n",
    "            if(len(Embedds[i])>=3000):\n",
    "            #if(len(Embedds)>=3000):\n",
    "                break\n",
    "    nmi=[]\n",
    "    for i in range(0,13):\n",
    "        kmeans = KMeans(n_clusters=4, random_state=0).fit(Embedds[i])\n",
    "        nmi.append(normalized_mutual_info_score(np.array(Tags),kmeans.labels_))\n",
    "    all_nmi.append(nmi)\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26615675, 0.33489916, 0.44400374, 0.50212925, 0.49766814,\n",
       "       0.47753819, 0.45593526, 0.40627995, 0.33742857, 0.34303634,\n",
       "       0.32778245, 0.28541799, 0.23972654])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(all_nmi),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "all_nmi =[]\n",
    "for n in range(0,10):\n",
    "    Tags, Embedds = [], [[] for i in range(0,13)]\n",
    "    #Tags, Embedds = [], []\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        _, x, is_heads_b, _, y_b, _ = batch\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            output=model(x)\n",
    "            embedds_b = output[2]\n",
    "            #embedds_b = output[0] # We want to test output embeddings\n",
    "            for i in range(0,13):\n",
    "                for embedds, is_heads, ys in zip(embedds_b[i].cpu().numpy(), is_heads_b, y_b):\n",
    "                    embbed = [hat for head, hat in zip(is_heads, embedds) if head == 1]   \n",
    "                    y = [hat.item() for head, hat in zip(is_heads, ys) if head == 1]  \n",
    "                    Embedds[i].extend(embbed[1:-1])\n",
    "                    if(i==0):\n",
    "                        Tags.extend(y[1:-1])\n",
    "            #for embedds, is_heads, ys in zip(embedds_b.cpu().numpy(), is_heads_b, y_b):\n",
    "                #embbed = [hat for head, hat in zip(is_heads, embedds) if head == 1]   \n",
    "                #y = [hat.item() for head, hat in zip(is_heads, ys) if head == 1]  \n",
    "                #Embedds.extend(embbed[1:-1])\n",
    "                #if(i==0):\n",
    "                    #Tags.extend(y[1:-1])\n",
    "                #Tags.extend(y[1:-1])\n",
    "            if(len(Embedds[i])>=3000):\n",
    "            #if(len(Embedds)>=3000):\n",
    "                break\n",
    "    nmi=[]\n",
    "    for i in range(0,13):\n",
    "        kmeans = KMeans(n_clusters=4, random_state=0).fit(Embedds[i])\n",
    "        nmi.append(normalized_mutual_info_score(np.array(Tags),kmeans.labels_))\n",
    "    all_nmi.append(nmi)\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18091549, 0.35020918, 0.38046554, 0.4527303 , 0.43411033,\n",
       "       0.51854905, 0.45723204, 0.37932747, 0.33658722, 0.31705283,\n",
       "       0.29594284, 0.16537291, 0.05815219])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(all_nmi),axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bots",
   "language": "python",
   "name": "bots"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
